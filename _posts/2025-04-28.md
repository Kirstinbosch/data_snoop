---
layout: post
title: "Day 14"
description: "My new blog"
date: 2025-04-28
categories: blog
theme: "jekyll-theme-minima"
url: "https://kirstinbosch.github.io/data_snoop"
baseurl: "/data_snoop"
---

Terms: expectations of Joint Distributions, linearity of expectation, independent random variables, conditional expectations, 

When we say 
𝐸(𝑋∣𝑌)=𝐸(𝑋)if 𝑋 and 𝑌 are independent, we are not saying that the value of 𝑋 stays the same. What we are saying is
The expected value (the average outcome) of 𝑋 doesn't change even if you know the value of 𝑌. So knowing 𝑌 tells you nothing new about 𝑋 because 𝑋 and 𝑌 are independent
𝐸(𝑋∣𝑌) is still a random variable that depends on Y. But if X and y are indepndent, then 𝐸(𝑋∣𝑌) becomes just a constant, it is simply E(X) - because Y gives no information about X. 

Variance:
--> the variance of a constant (non-random) quantity is zero since it has no variation.
--> Adding a constant to a random variable does not affect its variance, since we use the linearity of expectation.

