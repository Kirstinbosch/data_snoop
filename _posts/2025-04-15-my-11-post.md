---
layout: post
title: "Day XI"
description: "My new blog"
date: 2025-04-15
categories: blog
theme: "jekyll-theme-minima"
url: "https://kirstinbosch.github.io/data_snoop"
baseurl: "/data_snoop"
---

Bayes Theorem: "given that an outcome was observed, what's the probability that a particular cause led to it."
We are being asked the reverse conditional probability.
P(cause| observed outcome)

The big picture: 
--> the law of total probability, which involves partitioms of the sample space. Then we update the beliefs about the causes (partitions) based on new evidence

Cause → Effect: The Forward Thinking
Each health status (cause) has some probability of producing a positive test:
•	P(B∣A1): probability of a positive test given the person is healthy (maybe low, e.g. 0.01)
•	P(B∣A2): probability of a positive test given they are mildly ill (medium, e.g. 0.8)
•	P(B∣A3): probability of a positive test given they are severely ill (high, e.g. 0.99)
This is the natural, forward direction of thinking — "If I know the person’s health status, what’s the chance I get this test result?"
Bayes Flips It: Effect → Cause
But in Bayesian thinking, we reverse this.
We observe the test result (positive) — and now we ask:
“Given that I saw this test result (evidence), what is the probability that the person is in each health category?”
This is effect → cause:
We saw the effect (a positive test), now we update our belief in the possible causes (what health status the person actually has).


